from ..mcts import MCTS
from ..utils import play_game, node_to_probability_distribution

import numpy as np
import multiprocessing
import logwood


class StagedModelTrainer:
    def __init__(
        self,
        env,
        config,
        replay,
        evaluator,
        callbacks=[],
        model_dir=None,
        replay_dir=None,
    ):
        """Initializes a StagedModelTrainer.

        This class trains a neural-net guided by MCTS in three stages.
        - Stage 1: Data is generated by playing games with the MCTS.
        - Stage 2: The data that was generated is used to train a neural net.
        - Stage 3: The trained neural net and original neural net compete.
                   The model that wins this stage takes over guiding the neural net.
        
        Arguments:
            env {Environment} -- The game environment.
            config {dict} -- A configuration for the MCTS to run. Must contain a `model` key.
            replay {mcts.nn.replay.Replay} -- The replay table used to store data points.
            evaluator {mcts.evaluators.Evaluator} -- The evaluator to use during the evaluation stage.
        
        Keyword Arguments:
            callbacks {list} -- A list of Keras Callbacks to use in training (default: {[]})
            model_dir {str} -- A filepath to save models (default: {None})
            replay_dir {str} -- A filepath to save replay tables (default: {None})
        """

        self.environment = env
        self.config = config
        self.training_model = config["model"]
        self.generation_model = self.training_model.clone()
        self.savepath = model_dir
        self.replay_path = replay_dir
        self.callbacks = callbacks
        self.replay = replay
        self.evaluator = evaluator
        self._logger = logwood.get_logger(self.__class__.__name__)

    def train(
        self, epochs=10, generation_steps=100, training_steps=100, evaluation_steps=10
    ):
        """Trains the model in stages.
        
        Keyword Arguments:
            epochs {int} -- The number of times to iterate through the three-staged process (default: {10})
            generation_steps {int} -- The number of games played to generate data points during the generation stage. (default: {100})
            training_steps {int} -- The number of batches to train on. (default: {100})
            evaluation_steps {int} -- The number of evaluation games to play for model evaluation. (default: {10})
        """
        mcts = MCTS(self.environment)
        mcts.build(self.config)
        mcts.calculation_time = 1  # TODO: Allow this to be included in config
        for epoch in range(epochs):
            self._logger.info("Starting epoch {}".format(epoch))
            # Generate Data

            self._logger.info("Entering Generation Phase")
            for i in range(generation_steps):
                self._logger.info("Playing Generation Game {}".format(i))
                game_results, reward, winner = play_game(mcts)
                self._process_and_store(mcts, game_results, reward, winner)

            # Save replay table
            if self.replay_path:
                replay_savepath = "{}/replay{}".format(self.replay_path, epoch)
                self._logger.info("Saving Replay Table to {}".format(replay_savepath))
                self.replay.save(replay_savepath)

            # Train model
            self._logger.info("Entering Training Phase")
            self.train_batches(training_steps)

            # Evaluate Model
            self._logger.info("Entering Evaluation Phase")
            results = self.evaluator.evaluate(
                self.generation_model, self.training_model, games=evaluation_steps
            )

            if results.winner == "Challenger":
                self._logger.info("Challenger model wins - updating model...")
                self.generation_model.set_weights(self.training_model.get_weights())

                if self.savepath:
                    savepath = "{}/model{}".format(self.savepath, epoch)
                    self._logger.info("Saving Model to {}".format(savepath))
                    self.training_model.save(savepath)

            else:
                self._logger.info("Challenger model loses - no update performed")

    def train_batches(self, n_batches, batch_size=16):
        """Trains the model for a number of batches"""
        generator = self._make_generator(batch_size)
        validation_data = next(generator)

        # Since this is a reinforcement learning problem, validation data doesn't really mean anything.
        # However, we need to specify it for some callbacks (e.g tensorboard),
        # so we'll just randomly use some data points to approximate.
        self.training_model.fit_generator(
            generator,
            steps_per_epoch=1,
            epochs=n_batches,
            callbacks=self.callbacks,
            validation_data=validation_data,
        )

    def _make_generator(self, batch_size):
        def generator(batch_size):
            while True:
                input_states, action_values, rewards = self.replay.get_batch(batch_size)
                X, y = (
                    input_states,
                    {"policy_head": action_values, "value_head": rewards},
                )
                yield X, y

        return generator(batch_size)

    def _process_and_store(self, m, game_results, reward, winner):
        """Processes the data from the game results of an MCTS.
        Makes the data suitable for storage in the replay table.
        
        Arguments:
            m {mcts.mcts.MCTS} -- The MCTS object that gave the game results
            game_results {list} -- A list of nodes that were traversed in the game
        """

        n = self.replay.policy_size
        for node_id in game_results:
            node = m.tree.get_by_id(node_id)
            # Initialize policy values to zeros. This is the default
            # so invalid actions are always registered as zero.
            policy_values = np.zeros(n)
            # The probability distribution of selecting actions
            # in the current node based on visit count.
            pi = node_to_probability_distribution(node)

            # All valid actions are contained in the edge keys
            valid_actions = list(node.edges)
            # Retrive only valid entries of the probability distribution
            ix = np.isin(pi[:, 0], valid_actions)
            # Store the probability distributions in the policy values
            # where the action is valid
            policy_values[valid_actions] = pi[ix, 1]

            if node.player != winner:
                r = -reward
            else:
                r = reward

            self.replay.add_data(node.state, policy_values, r)
